#' @import np dplyr MASS
#' @export
#'
#'
TMLE <- function(a,data,treatment, mediators, outcome, covariates,
onestep=T, mediator.method=c("bayes","densratio","np"), superlearner=F,
lib = c("SL.glm","SL.bayesglm", "SL.gam","SL.earth","SL.ranger","SL.svm","SL.mean"), n.iter=500, eps=T, cvg.criteria=0.01){
# sample size
n <- nrow(data)
if (is.vector(a) & length(a)>2){ ## Invalid input ==
print("Invalid input. Enter a=c(1,0) for Average Causal Effect estimation. Enter a=1 or a=0 for average counterfactual outcome estimation at the specified treatment level.")
}else if (is.vector(a) & length(a)==2){ ## ATE estimate ==
## TMLE estimator
out.a1 <- TMLE.all(a=a[1],data=data,treatment=treatment, mediators=mediators, outcome=outcome, covariates=covariates,
onestep=onestep, mediator.method=mediator.method, superlearner=superlearner,
lib = c("SL.glm","SL.bayesglm", "SL.gam","SL.earth","SL.ranger","SL.svm","SL.mean"), n.iter=500, eps=T, cvg.criteria=0.01)
out.a0 <- TMLE.all(a=a[2],data=data,treatment=treatment, mediators=mediators, outcome=outcome, covariates=covariates,
onestep=onestep, mediator.method=mediator.method, superlearner=superlearner,
lib = c("SL.glm","SL.bayesglm", "SL.gam","SL.earth","SL.ranger","SL.svm","SL.mean"), n.iter=500, eps=T, cvg.criteria=0.01)
# run TMLE
tmle_output_Y1 <- out.a1$TMLE
tmle_output_Y0 <- out.a0$TMLE
# estimate E[Y(1)], E[Y(0)], and ATE
hat_E.Y1 = tmle_output_Y1$estimated_psi
hat_E.Y0 = tmle_output_Y0$estimated_psi
hat_ATE = hat_E.Y1 - hat_E.Y0
# lower CI
lower.ci_ATE = hat_ATE - 1.96*sqrt(mean((tmle_output_Y1$EIF-tmle_output_Y0$EIF)^2)/n)
# upper CI
upper.ci_ATE = hat_ATE + 1.96*sqrt(mean((tmle_output_Y1$EIF-tmle_output_Y0$EIF)^2)/n)
tmle.out <- list(ATE=hat_ATE, # estimated parameter
lower.ci=lower.ci, # lower bound of 95% CI
upper.ci=upper.ci, # upper bound of 95% CI
EIF=tmle_output_Y1$EIF-tmle_output_Y0$EIF # EIF
)
if (onestep==T){
# run TMLE
onestep_output_Y1 <- out.a1$Onestep
onestep_output_Y0 <- out.a0$Onestep
# estimate E[Y(1)], E[Y(0)], and ATE
hat_E.Y1 = onestep_output_Y1$estimated_psi
hat_E.Y0 = onestep_output_Y0$estimated_psi
hat_ATE = hat_E.Y1 - hat_E.Y0
# lower CI
lower.ci_ATE = hat_ATE - 1.96*sqrt(mean((onestep_output_Y1$EIF-onestep_output_Y0$EIF)^2)/n)
# upper CI
upper.ci_ATE = hat_ATE + 1.96*sqrt(mean((onestep_output_Y1$EIF-onestep_output_Y0$EIF)^2)/n)
onestep.out <- list(ATE=hat_ATE, # estimated parameter
lower.ci=lower.ci, # lower bound of 95% CI
upper.ci=upper.ci, # upper bound of 95% CI
EIF=onestep_output_Y1$EIF-onestep_output_Y0$EIF # EIF
)
return(list(TMLE=tmle.out,Onestep=onestep.out, TMLE.Y1=tmle_output_Y1, TMLE.Y0 = tmle_output_Y0, Onestep.Y1=onestep_output_Y1, Onestep.Y0=onestep_output_Y0))
}else {return(list(TMLE=tmle.out,TMLE.Y1=tmle_output_Y1, TMLE.Y0 = tmle_output_Y0))}
}else if (a==1) { ## E(Y^1) estimate ==
out.a1 <- TMLE.all(a=1,data=data,treatment=treatment, mediators=mediators, outcome=outcome, covariates=covariates,
onestep=onestep, mediator.method=mediator.method, superlearner=superlearner,
lib = lib, n.iter=500, eps=T, cvg.criteria=0.01)
return(out.a1)
}else if (a==0){ ## E(Y^0) estimate ==
out.a0 <- TMLE.all(a=0,data=data,treatment=treatment, mediators=mediators, outcome=outcome, covariates=covariates,
onestep=onestep, mediator.method=mediator.method, superlearner=superlearner,
lib = lib, n.iter=500, eps=T, cvg.criteria=0.01)
return(out.a0)
}
}
test <- TMLE(a=c(1,0),data,treatment="A", mediators="M", outcome="Y", covariates=c("X1","X2") ,mediator.method = "bayes" ,onestep=T, superlearner=T, lib = c("SL.glm","SL.bayesglm", "SL.gam","SL.earth","SL.ranger","SL.svm","SL.mean"), n.iter=500, eps=T, cvg.criteria=0.01)
test$TMLE$ATE
test$Onestep$ATE
#'       \item{\code{EDstar_ps.vec}}{A vector containing the average value of the mapping of EIF in tangent space \eqn{A|X} over iterations. This is useful for checking the convergence behavior of the propensity score. It's expected to be close to 0 when convergence is achieved.}
#'       \item{\code{eps2_vec}}{A vector containing the index for submodels of the mediator density over iterations. This is useful for checking the convergence behavior of the mediator density. It's expected to be close to 0 when convergence is achieved.}
#'       \item{\code{eps3_vec}}{A vector containing the index for submodels of the propensity score over iterations. This is useful for checking the convergence behavior of the propensity score. It's expected to be close to 0 when convergence is achieved.}
#'       \item{\code{iter}}{Number of iterations where convergence is achieved for the iterative update of the mediator density and propensity score.}
#'       }
#' @examples TMLE.all(a,data,treatment, mediators, outcome, covariates,onestep=T, mediator.method=c("bayes","densratio","np"), superlearner=F,lib = c("SL.glm","SL.bayesglm", "SL.gam","SL.earth","SL.ranger","SL.svm","SL.mean"), n.iter=500, eps=T, cvg.criteria=0.01)
#' @import np dplyr MASS
#' @export
#'
#'
TMLE <- function(a,data,treatment, mediators, outcome, covariates,
onestep=T, mediator.method=c("bayes","densratio","np"), superlearner=F,
lib = c("SL.glm","SL.bayesglm", "SL.gam","SL.earth","SL.ranger","SL.svm","SL.mean"), n.iter=500, eps=T, cvg.criteria=0.01){
# sample size
n <- nrow(data)
if (is.vector(a) & length(a)>2){ ## Invalid input ==
print("Invalid input. Enter a=c(1,0) for Average Causal Effect estimation. Enter a=1 or a=0 for average counterfactual outcome estimation at the specified treatment level.")
}else if (is.vector(a) & length(a)==2){ ## ATE estimate ==
## TMLE estimator
out.a1 <- TMLE.all(a=a[1],data=data,treatment=treatment, mediators=mediators, outcome=outcome, covariates=covariates,
onestep=onestep, mediator.method=mediator.method, superlearner=superlearner,
lib = lib, n.iter=500, eps=T, cvg.criteria=0.01)
out.a0 <- TMLE.all(a=a[2],data=data,treatment=treatment, mediators=mediators, outcome=outcome, covariates=covariates,
onestep=onestep, mediator.method=mediator.method, superlearner=superlearner,
lib = lib, n.iter=500, eps=T, cvg.criteria=0.01)
# run TMLE
tmle_output_Y1 <- out.a1$TMLE
tmle_output_Y0 <- out.a0$TMLE
# estimate E[Y(1)], E[Y(0)], and ATE
hat_E.Y1 = tmle_output_Y1$estimated_psi
hat_E.Y0 = tmle_output_Y0$estimated_psi
hat_ATE = hat_E.Y1 - hat_E.Y0
# lower CI
lower.ci_ATE = hat_ATE - 1.96*sqrt(mean((tmle_output_Y1$EIF-tmle_output_Y0$EIF)^2)/n)
# upper CI
upper.ci_ATE = hat_ATE + 1.96*sqrt(mean((tmle_output_Y1$EIF-tmle_output_Y0$EIF)^2)/n)
tmle.out <- list(ATE=hat_ATE, # estimated parameter
lower.ci=lower.ci, # lower bound of 95% CI
upper.ci=upper.ci, # upper bound of 95% CI
EIF=tmle_output_Y1$EIF-tmle_output_Y0$EIF # EIF
)
if (onestep==T){
# run TMLE
onestep_output_Y1 <- out.a1$Onestep
onestep_output_Y0 <- out.a0$Onestep
# estimate E[Y(1)], E[Y(0)], and ATE
hat_E.Y1 = onestep_output_Y1$estimated_psi
hat_E.Y0 = onestep_output_Y0$estimated_psi
hat_ATE = hat_E.Y1 - hat_E.Y0
# lower CI
lower.ci_ATE = hat_ATE - 1.96*sqrt(mean((onestep_output_Y1$EIF-onestep_output_Y0$EIF)^2)/n)
# upper CI
upper.ci_ATE = hat_ATE + 1.96*sqrt(mean((onestep_output_Y1$EIF-onestep_output_Y0$EIF)^2)/n)
onestep.out <- list(ATE=hat_ATE, # estimated parameter
lower.ci=lower.ci, # lower bound of 95% CI
upper.ci=upper.ci, # upper bound of 95% CI
EIF=onestep_output_Y1$EIF-onestep_output_Y0$EIF # EIF
)
print(paste0("TMLE estimated ATE: ",tmle.out$ATE,"; 95% CI: (",tmle.out$lower.ci,", ",tmle.out$upper.ci,")\n","Onestep estimated ATE: ",onestep.out$ATE,"; 95% CI: (",onestep.out$lower.ci,", ",onestep.out$upper.ci,")"))
return(list(TMLE=tmle.out,Onestep=onestep.out, TMLE.Y1=tmle_output_Y1, TMLE.Y0 = tmle_output_Y0, Onestep.Y1=onestep_output_Y1, Onestep.Y0=onestep_output_Y0))
}else {
print(paste0("TMLE estimated ATE: ",tmle.out$ATE,"; 95% CI: (",tmle.out$lower.ci,", ",tmle.out$upper.ci,")"))
return(list(TMLE=tmle.out,TMLE.Y1=tmle_output_Y1, TMLE.Y0 = tmle_output_Y0))
}
}else if (a==1) { ## E(Y^1) estimate ==
out.a1 <- TMLE.all(a=1,data=data,treatment=treatment, mediators=mediators, outcome=outcome, covariates=covariates,
onestep=onestep, mediator.method=mediator.method, superlearner=superlearner,
lib = lib, n.iter=500, eps=T, cvg.criteria=0.01)
return(out.a1)
}else if (a==0){ ## E(Y^0) estimate ==
out.a0 <- TMLE.all(a=0,data=data,treatment=treatment, mediators=mediators, outcome=outcome, covariates=covariates,
onestep=onestep, mediator.method=mediator.method, superlearner=superlearner,
lib = lib, n.iter=500, eps=T, cvg.criteria=0.01)
return(out.a0)
}
}
test <- TMLE(a=c(1,0),data,treatment="A", mediators="M", outcome="Y", covariates=c("X1","X2") ,mediator.method = "bayes" ,onestep=T, superlearner=T, lib = c("SL.glm","SL.bayesglm", "SL.gam","SL.earth","SL.ranger","SL.svm","SL.mean"), n.iter=500, eps=T, cvg.criteria=0.01)
#'       \item{\code{EDstar_ps.vec}}{A vector containing the average value of the mapping of EIF in tangent space \eqn{A|X} over iterations. This is useful for checking the convergence behavior of the propensity score. It's expected to be close to 0 when convergence is achieved.}
#'       \item{\code{eps2_vec}}{A vector containing the index for submodels of the mediator density over iterations. This is useful for checking the convergence behavior of the mediator density. It's expected to be close to 0 when convergence is achieved.}
#'       \item{\code{eps3_vec}}{A vector containing the index for submodels of the propensity score over iterations. This is useful for checking the convergence behavior of the propensity score. It's expected to be close to 0 when convergence is achieved.}
#'       \item{\code{iter}}{Number of iterations where convergence is achieved for the iterative update of the mediator density and propensity score.}
#'       }
#' @examples TMLE.all(a,data,treatment, mediators, outcome, covariates,onestep=T, mediator.method=c("bayes","densratio","np"), superlearner=F,lib = c("SL.glm","SL.bayesglm", "SL.gam","SL.earth","SL.ranger","SL.svm","SL.mean"), n.iter=500, eps=T, cvg.criteria=0.01)
#' @import np dplyr MASS
#' @export
#'
#'
TMLE <- function(a,data,treatment, mediators, outcome, covariates,
onestep=T, mediator.method=c("bayes","densratio","np"), superlearner=F,
lib = c("SL.glm","SL.bayesglm", "SL.gam","SL.earth","SL.ranger","SL.svm","SL.mean"), n.iter=500, eps=T, cvg.criteria=0.01){
# sample size
n <- nrow(data)
if (is.vector(a) & length(a)>2){ ## Invalid input ==
print("Invalid input. Enter a=c(1,0) for Average Causal Effect estimation. Enter a=1 or a=0 for average counterfactual outcome estimation at the specified treatment level.")
}else if (is.vector(a) & length(a)==2){ ## ATE estimate ==
## TMLE estimator
out.a1 <- TMLE.all(a=a[1],data=data,treatment=treatment, mediators=mediators, outcome=outcome, covariates=covariates,
onestep=onestep, mediator.method=mediator.method, superlearner=superlearner,
lib = lib, n.iter=500, eps=T, cvg.criteria=0.01)
out.a0 <- TMLE.all(a=a[2],data=data,treatment=treatment, mediators=mediators, outcome=outcome, covariates=covariates,
onestep=onestep, mediator.method=mediator.method, superlearner=superlearner,
lib = lib, n.iter=500, eps=T, cvg.criteria=0.01)
# run TMLE
tmle_output_Y1 <- out.a1$TMLE
tmle_output_Y0 <- out.a0$TMLE
# estimate E[Y(1)], E[Y(0)], and ATE
hat_E.Y1 = tmle_output_Y1$estimated_psi
hat_E.Y0 = tmle_output_Y0$estimated_psi
hat_ATE = hat_E.Y1 - hat_E.Y0
# lower CI
lower.ci_ATE = hat_ATE - 1.96*sqrt(mean((tmle_output_Y1$EIF-tmle_output_Y0$EIF)^2)/n)
# upper CI
upper.ci_ATE = hat_ATE + 1.96*sqrt(mean((tmle_output_Y1$EIF-tmle_output_Y0$EIF)^2)/n)
tmle.out <- list(ATE=hat_ATE, # estimated parameter
lower.ci=lower.ci, # lower bound of 95% CI
upper.ci=upper.ci, # upper bound of 95% CI
EIF=tmle_output_Y1$EIF-tmle_output_Y0$EIF # EIF
)
if (onestep==T){
# run TMLE
onestep_output_Y1 <- out.a1$Onestep
onestep_output_Y0 <- out.a0$Onestep
# estimate E[Y(1)], E[Y(0)], and ATE
hat_E.Y1 = onestep_output_Y1$estimated_psi
hat_E.Y0 = onestep_output_Y0$estimated_psi
hat_ATE = hat_E.Y1 - hat_E.Y0
# lower CI
lower.ci_ATE = hat_ATE - 1.96*sqrt(mean((onestep_output_Y1$EIF-onestep_output_Y0$EIF)^2)/n)
# upper CI
upper.ci_ATE = hat_ATE + 1.96*sqrt(mean((onestep_output_Y1$EIF-onestep_output_Y0$EIF)^2)/n)
onestep.out <- list(ATE=hat_ATE, # estimated parameter
lower.ci=lower.ci, # lower bound of 95% CI
upper.ci=upper.ci, # upper bound of 95% CI
EIF=onestep_output_Y1$EIF-onestep_output_Y0$EIF # EIF
)
print(paste0("TMLE estimated ACE: ",round(tmle.out$ATE,2),"; 95% CI: (",round(tmle.out$lower.ci,2),", ",round(tmle.out$upper.ci,2),") \n","Onestep estimated ACE: ",round(onestep.out$ATE,2),"; 95% CI: (",round(onestep.out$lower.ci,2),", ",round(onestep.out$upper.ci,2),")"))
return(list(TMLE=tmle.out,Onestep=onestep.out, TMLE.Y1=tmle_output_Y1, TMLE.Y0 = tmle_output_Y0, Onestep.Y1=onestep_output_Y1, Onestep.Y0=onestep_output_Y0))
}else {
print(paste0("TMLE estimated ATE: ",tmle.out$ATE,"; 95% CI: (",tmle.out$lower.ci,", ",tmle.out$upper.ci,")"))
return(list(TMLE=tmle.out,TMLE.Y1=tmle_output_Y1, TMLE.Y0 = tmle_output_Y0))
}
}else if (a==1) { ## E(Y^1) estimate ==
out.a1 <- TMLE.all(a=1,data=data,treatment=treatment, mediators=mediators, outcome=outcome, covariates=covariates,
onestep=onestep, mediator.method=mediator.method, superlearner=superlearner,
lib = lib, n.iter=500, eps=T, cvg.criteria=0.01)
return(out.a1)
}else if (a==0){ ## E(Y^0) estimate ==
out.a0 <- TMLE.all(a=0,data=data,treatment=treatment, mediators=mediators, outcome=outcome, covariates=covariates,
onestep=onestep, mediator.method=mediator.method, superlearner=superlearner,
lib = lib, n.iter=500, eps=T, cvg.criteria=0.01)
return(out.a0)
}
}
#'       \item{\code{EDstar_ps.vec}}{A vector containing the average value of the mapping of EIF in tangent space \eqn{A|X} over iterations. This is useful for checking the convergence behavior of the propensity score. It's expected to be close to 0 when convergence is achieved.}
#'       \item{\code{eps2_vec}}{A vector containing the index for submodels of the mediator density over iterations. This is useful for checking the convergence behavior of the mediator density. It's expected to be close to 0 when convergence is achieved.}
#'       \item{\code{eps3_vec}}{A vector containing the index for submodels of the propensity score over iterations. This is useful for checking the convergence behavior of the propensity score. It's expected to be close to 0 when convergence is achieved.}
#'       \item{\code{iter}}{Number of iterations where convergence is achieved for the iterative update of the mediator density and propensity score.}
#'       }
#' @examples TMLE.all(a,data,treatment, mediators, outcome, covariates,onestep=T, mediator.method=c("bayes","densratio","np"), superlearner=F,lib = c("SL.glm","SL.bayesglm", "SL.gam","SL.earth","SL.ranger","SL.svm","SL.mean"), n.iter=500, eps=T, cvg.criteria=0.01)
#' @import np dplyr MASS
#' @export
#'
#'
TMLE <- function(a,data,treatment, mediators, outcome, covariates,
onestep=T, mediator.method=c("bayes","densratio","np"), superlearner=F,
lib = c("SL.glm","SL.bayesglm", "SL.gam","SL.earth","SL.ranger","SL.svm","SL.mean"), n.iter=500, eps=T, cvg.criteria=0.01){
# sample size
n <- nrow(data)
if (is.vector(a) & length(a)>2){ ## Invalid input ==
print("Invalid input. Enter a=c(1,0) for Average Causal Effect estimation. Enter a=1 or a=0 for average counterfactual outcome estimation at the specified treatment level.")
}else if (is.vector(a) & length(a)==2){ ## ATE estimate ==
## TMLE estimator
out.a1 <- TMLE.all(a=a[1],data=data,treatment=treatment, mediators=mediators, outcome=outcome, covariates=covariates,
onestep=onestep, mediator.method=mediator.method, superlearner=superlearner,
lib = lib, n.iter=500, eps=T, cvg.criteria=0.01)
out.a0 <- TMLE.all(a=a[2],data=data,treatment=treatment, mediators=mediators, outcome=outcome, covariates=covariates,
onestep=onestep, mediator.method=mediator.method, superlearner=superlearner,
lib = lib, n.iter=500, eps=T, cvg.criteria=0.01)
# run TMLE
tmle_output_Y1 <- out.a1$TMLE
tmle_output_Y0 <- out.a0$TMLE
# estimate E[Y(1)], E[Y(0)], and ATE
hat_E.Y1 = tmle_output_Y1$estimated_psi
hat_E.Y0 = tmle_output_Y0$estimated_psi
hat_ATE = hat_E.Y1 - hat_E.Y0
# lower CI
lower.ci_ATE = hat_ATE - 1.96*sqrt(mean((tmle_output_Y1$EIF-tmle_output_Y0$EIF)^2)/n)
# upper CI
upper.ci_ATE = hat_ATE + 1.96*sqrt(mean((tmle_output_Y1$EIF-tmle_output_Y0$EIF)^2)/n)
tmle.out <- list(ATE=hat_ATE, # estimated parameter
lower.ci=lower.ci, # lower bound of 95% CI
upper.ci=upper.ci, # upper bound of 95% CI
EIF=tmle_output_Y1$EIF-tmle_output_Y0$EIF # EIF
)
if (onestep==T){
# run TMLE
onestep_output_Y1 <- out.a1$Onestep
onestep_output_Y0 <- out.a0$Onestep
# estimate E[Y(1)], E[Y(0)], and ATE
hat_E.Y1 = onestep_output_Y1$estimated_psi
hat_E.Y0 = onestep_output_Y0$estimated_psi
hat_ATE = hat_E.Y1 - hat_E.Y0
# lower CI
lower.ci_ATE = hat_ATE - 1.96*sqrt(mean((onestep_output_Y1$EIF-onestep_output_Y0$EIF)^2)/n)
# upper CI
upper.ci_ATE = hat_ATE + 1.96*sqrt(mean((onestep_output_Y1$EIF-onestep_output_Y0$EIF)^2)/n)
onestep.out <- list(ATE=hat_ATE, # estimated parameter
lower.ci=lower.ci, # lower bound of 95% CI
upper.ci=upper.ci, # upper bound of 95% CI
EIF=onestep_output_Y1$EIF-onestep_output_Y0$EIF # EIF
)
print(paste0("TMLE estimated ACE: ",round(tmle.out$ATE,2),"; 95% CI: (",round(tmle.out$lower.ci,2),", ",round(tmle.out$upper.ci,2),") \n","Onestep estimated ACE: ",round(onestep.out$ATE,2),"; 95% CI: (",round(onestep.out$lower.ci,2),", ",round(onestep.out$upper.ci,2),")"))
return(list(TMLE=tmle.out,Onestep=onestep.out, TMLE.Y1=tmle_output_Y1, TMLE.Y0 = tmle_output_Y0, Onestep.Y1=onestep_output_Y1, Onestep.Y0=onestep_output_Y0))
}else {
print(paste0("TMLE estimated ATE: ",round(tmle.out$ATE,2),"; 95% CI: (",round(tmle.out$lower.ci,2),", ",round(tmle.out$upper.ci,2),")"))
return(list(TMLE=tmle.out,TMLE.Y1=tmle_output_Y1, TMLE.Y0 = tmle_output_Y0))
}
}else if (a==1) { ## E(Y^1) estimate ==
out.a1 <- TMLE.all(a=1,data=data,treatment=treatment, mediators=mediators, outcome=outcome, covariates=covariates,
onestep=onestep, mediator.method=mediator.method, superlearner=superlearner,
lib = lib, n.iter=500, eps=T, cvg.criteria=0.01)
return(out.a1)
}else if (a==0){ ## E(Y^0) estimate ==
out.a0 <- TMLE.all(a=0,data=data,treatment=treatment, mediators=mediators, outcome=outcome, covariates=covariates,
onestep=onestep, mediator.method=mediator.method, superlearner=superlearner,
lib = lib, n.iter=500, eps=T, cvg.criteria=0.01)
return(out.a0)
}
}
test <- TMLE(a=c(1,0),data,treatment="A", mediators="M", outcome="Y", covariates=c("X1","X2") ,mediator.method = "bayes" ,onestep=T, superlearner=T, lib = c("SL.glm","SL.bayesglm", "SL.gam","SL.earth","SL.ranger","SL.svm","SL.mean"), n.iter=500, eps=T, cvg.criteria=0.01)
#'       \item{\code{EDstar_ps.vec}}{A vector containing the average value of the mapping of EIF in tangent space \eqn{A|X} over iterations. This is useful for checking the convergence behavior of the propensity score. It's expected to be close to 0 when convergence is achieved.}
#'       \item{\code{eps2_vec}}{A vector containing the index for submodels of the mediator density over iterations. This is useful for checking the convergence behavior of the mediator density. It's expected to be close to 0 when convergence is achieved.}
#'       \item{\code{eps3_vec}}{A vector containing the index for submodels of the propensity score over iterations. This is useful for checking the convergence behavior of the propensity score. It's expected to be close to 0 when convergence is achieved.}
#'       \item{\code{iter}}{Number of iterations where convergence is achieved for the iterative update of the mediator density and propensity score.}
#'       }
#' @examples TMLE.all(a,data,treatment, mediators, outcome, covariates,onestep=T, mediator.method=c("bayes","densratio","np"), superlearner=F,lib = c("SL.glm","SL.bayesglm", "SL.gam","SL.earth","SL.ranger","SL.svm","SL.mean"), n.iter=500, eps=T, cvg.criteria=0.01)
#' @import np dplyr MASS
#' @export
#'
#'
TMLE <- function(a,data,treatment, mediators, outcome, covariates,
onestep=T, mediator.method=c("bayes","densratio","np"), superlearner=F,
lib = c("SL.glm","SL.bayesglm", "SL.gam","SL.earth","SL.ranger","SL.svm","SL.mean"), n.iter=500, eps=T, cvg.criteria=0.01){
# sample size
n <- nrow(data)
if (is.vector(a) & length(a)>2){ ## Invalid input ==
print("Invalid input. Enter a=c(1,0) for Average Causal Effect estimation. Enter a=1 or a=0 for average counterfactual outcome estimation at the specified treatment level.")
}else if (is.vector(a) & length(a)==2){ ## ATE estimate ==
## TMLE estimator
out.a1 <- TMLE.all(a=a[1],data=data,treatment=treatment, mediators=mediators, outcome=outcome, covariates=covariates,
onestep=onestep, mediator.method=mediator.method, superlearner=superlearner,
lib = lib, n.iter=500, eps=T, cvg.criteria=0.01)
out.a0 <- TMLE.all(a=a[2],data=data,treatment=treatment, mediators=mediators, outcome=outcome, covariates=covariates,
onestep=onestep, mediator.method=mediator.method, superlearner=superlearner,
lib = lib, n.iter=500, eps=T, cvg.criteria=0.01)
# run TMLE
tmle_output_Y1 <- out.a1$TMLE
tmle_output_Y0 <- out.a0$TMLE
# estimate E[Y(1)], E[Y(0)], and ATE
hat_E.Y1 = tmle_output_Y1$estimated_psi
hat_E.Y0 = tmle_output_Y0$estimated_psi
hat_ATE = hat_E.Y1 - hat_E.Y0
# lower CI
lower.ci_ATE = hat_ATE - 1.96*sqrt(mean((tmle_output_Y1$EIF-tmle_output_Y0$EIF)^2)/n)
# upper CI
upper.ci_ATE = hat_ATE + 1.96*sqrt(mean((tmle_output_Y1$EIF-tmle_output_Y0$EIF)^2)/n)
tmle.out <- list(ATE=hat_ATE, # estimated parameter
lower.ci=lower.ci, # lower bound of 95% CI
upper.ci=upper.ci, # upper bound of 95% CI
EIF=tmle_output_Y1$EIF-tmle_output_Y0$EIF # EIF
)
if (onestep==T){
# run TMLE
onestep_output_Y1 <- out.a1$Onestep
onestep_output_Y0 <- out.a0$Onestep
# estimate E[Y(1)], E[Y(0)], and ATE
hat_E.Y1 = onestep_output_Y1$estimated_psi
hat_E.Y0 = onestep_output_Y0$estimated_psi
hat_ATE = hat_E.Y1 - hat_E.Y0
# lower CI
lower.ci_ATE = hat_ATE - 1.96*sqrt(mean((onestep_output_Y1$EIF-onestep_output_Y0$EIF)^2)/n)
# upper CI
upper.ci_ATE = hat_ATE + 1.96*sqrt(mean((onestep_output_Y1$EIF-onestep_output_Y0$EIF)^2)/n)
onestep.out <- list(ATE=hat_ATE, # estimated parameter
lower.ci=lower.ci, # lower bound of 95% CI
upper.ci=upper.ci, # upper bound of 95% CI
EIF=onestep_output_Y1$EIF-onestep_output_Y0$EIF # EIF
)
cat(paste0("TMLE estimated ACE: ",round(tmle.out$ATE,2),"; 95% CI: (",round(tmle.out$lower.ci,2),", ",round(tmle.out$upper.ci,2),") \n","Onestep estimated ACE: ",round(onestep.out$ATE,2),"; 95% CI: (",round(onestep.out$lower.ci,2),", ",round(onestep.out$upper.ci,2),")"))
return(list(TMLE=tmle.out,Onestep=onestep.out, TMLE.Y1=tmle_output_Y1, TMLE.Y0 = tmle_output_Y0, Onestep.Y1=onestep_output_Y1, Onestep.Y0=onestep_output_Y0))
}else {
cat(paste0("TMLE estimated ATE: ",round(tmle.out$ATE,2),"; 95% CI: (",round(tmle.out$lower.ci,2),", ",round(tmle.out$upper.ci,2),")"))
return(list(TMLE=tmle.out,TMLE.Y1=tmle_output_Y1, TMLE.Y0 = tmle_output_Y0))
}
}else if (a==1) { ## E(Y^1) estimate ==
out.a1 <- TMLE.all(a=1,data=data,treatment=treatment, mediators=mediators, outcome=outcome, covariates=covariates,
onestep=onestep, mediator.method=mediator.method, superlearner=superlearner,
lib = lib, n.iter=500, eps=T, cvg.criteria=0.01)
return(out.a1)
}else if (a==0){ ## E(Y^0) estimate ==
out.a0 <- TMLE.all(a=0,data=data,treatment=treatment, mediators=mediators, outcome=outcome, covariates=covariates,
onestep=onestep, mediator.method=mediator.method, superlearner=superlearner,
lib = lib, n.iter=500, eps=T, cvg.criteria=0.01)
return(out.a0)
}
}
test <- TMLE(a=c(1,0),data,treatment="A", mediators="M", outcome="Y", covariates=c("X1","X2") ,mediator.method = "bayes" ,onestep=T, superlearner=T, lib = c("SL.glm","SL.bayesglm", "SL.gam","SL.earth","SL.ranger","SL.svm","SL.mean"), n.iter=500, eps=T, cvg.criteria=0.01)
load("/Users/apple/Library/CloudStorage/Dropbox/Front-door_Anna/code/continuous.Rdata")
library(ggplot2)
library(ggpubr)
library(latex2exp)
library(reshape2)
library(stats)
library(haldensify)
library(np)
library(xtable)
library(SuperLearner)
library(densratio)
library(MASS)
library(mvtnorm)
library(TmleFrontdoor)
treatment="A"; mediator="M";outcome="Y"; covariates="X"
lib = c("SL.glm", "SL.gam",
"SL.ranger",
"SL.svm","SL.mean")
# Variables
A <- data[,treatment]
M <- data[,mediators, drop = F]
X <- data[,covariates,drop = F]
Y <- data[,outcome]
dat_MAX <- data.frame(M,A=A,X)
dat_MaX <- data.frame(M,A=a,X)
mediators="M"
M <- data[,mediators, drop = F]
X <- data[,covariates,drop = F]
Y <- data[,outcome]
dat_MAX <- data.frame(M,A=A,X)
a=1
dat_MaX <- data.frame(M,A=a,X)
# new data sets
data_Aa = data.frame(M, A=A, X)
data_A1 = data.frame(M, A=1, X)
data_A0 = data.frame(M, A=0, X)
or_fit <- CV.SuperLearner(Y=Y, X=dat_MAX, family = gaussian(), V = K, SL.library = lib, control = list(saveFitLibrary=T),saveAll = T)
K=5
or_fit <- CV.SuperLearner(Y=Y, X=dat_MAX, family = gaussian(), V = K, SL.library = lib, control = list(saveFitLibrary=T),saveAll = T)
or_pred <- or_fit$SL.predict
or_pred_a1 <- unlist(lapply(1:K, function(x) predict(or_fit$AllSL[[x]], newdata=data_A1[or_fit$folds[[x]],])[[1]] %>% as.vector()))[order(unlist(lapply(1:K, function(x) or_fit$folds[[x]])))]
or_pred_a0 <- unlist(lapply(1:K, function(x) predict(or_fit$AllSL[[x]], newdata=data_A0[or_fit$folds[[x]],])[[1]] %>% as.vector()))[order(unlist(lapply(1:K, function(x) or_fit$folds[[x]])))]
generate_data <- function(n,parA = c(-1,1), parU=c(1,1,1,0), parM = c(1,1,1,0), parY = c(1, 1, 1, 0), sd.M=1, sd.U=1, sd.Y=1){
X <- runif(n, 0, 1) # p(X)
A <- rbinom(n, 1, plogis(parA[1] + parA[2]*X)) # p(A|X)
U <- parU[1] + parU[2]*A + parU[3]*X + parU[4]*A*X + rnorm(n,0,sd.U) # p(U|A,X)
M <- parM[1] + parM[2]*A + parM[3]*X + parM[4]*A*X + rnorm(n,0,sd.M) # p(M|A,X)
Y <- parY[1]*U + parY[2]*M + parY[3]*X + parY[4]*M*X + rnorm(n, 0, sd.Y) # p(Y|U,M,X)
data <- data.frame(X=X, U=U, A=A, M=M, Y=Y, AX=A*X, MX=M*X)
# propensity score
ps <- A*plogis(parA[1] + parA[2]*X)+(1-A)*(1-plogis(parA[1] + parA[2]*X))
# mediator density ratio: p(M|a,X)/p(M|A,X)
m.ratio.a1 <- dnorm(M,parM[1] + parM[2]*1 + parM[3]*X + parM[4]*1*X,sd.M)/dnorm(M,parM[1] + parM[2]*A + parM[3]*X + parM[4]*A*X,sd.M)
m.ratio.a0 <- dnorm(M,parM[1] + parM[2]*0 + parM[3]*X + parM[4]*0*X,sd.M)/dnorm(M,parM[1] + parM[2]*A + parM[3]*X + parM[4]*A*X,sd.M)
return(list(data = data,
parA=parA,
parU=parU,
parM=parM,
parY=parY,
sd.U=sd.U,
sd.Y=sd.Y,
sd.M=sd.M,
ps=ps,
m.ratio.a1=m.ratio.a1,
m.ratio.a0=m.ratio.a0))
}
n=1000
# generate data
dat_output = generate_data(n)
data = dat_output$data
attach(data, warn.conflicts=FALSE)
save(list=c("data"), file="/Users/apple/Library/CloudStorage/Dropbox/Front-door_Anna/code/0-continuous.Rdata")
# Variables
A <- data[,treatment]
M <- data[,mediators, drop = F]
X <- data[,covariates,drop = F]
Y <- data[,outcome]
dat_MAX <- data.frame(M,A=A,X)
dat_MaX <- data.frame(M,A=a,X)
# new data sets
data_Aa = data.frame(M, A=A, X)
data_A1 = data.frame(M, A=1, X)
data_A0 = data.frame(M, A=0, X)
or_fit <- CV.SuperLearner(Y=Y, X=dat_MAX, family = gaussian(), V = K, SL.library = lib, control = list(saveFitLibrary=T),saveAll = T)
or_pred <- or_fit$SL.predict
or_pred_a1 <- unlist(lapply(1:K, function(x) predict(or_fit$AllSL[[x]], newdata=data_A1[or_fit$folds[[x]],])[[1]] %>% as.vector()))[order(unlist(lapply(1:K, function(x) or_fit$folds[[x]])))]
or_pred_a0 <- unlist(lapply(1:K, function(x) predict(or_fit$AllSL[[x]], newdata=data_A0[or_fit$folds[[x]],])[[1]] %>% as.vector()))[order(unlist(lapply(1:K, function(x) or_fit$folds[[x]])))]
lm <- lm(Y~M+A+X)
lm <- lm(Y~.,data=dat_MAX)
lm_pred <- predict(lm)
cv_pred <- or_pred
or_fit <- SuperLearner(Y=Y, X=dat_MAX, family = gaussian(),SL.library = lib)
or_pred <- predict(or_fit)[[1]] %>% as.vector()
or_pred_a1 <- predict(or_fit, newdata=data_A1)[[1]] %>% as.vector()
or_pred_a0 <- predict(or_fit, newdata=data_A0)[[1]] %>% as.vector()
sl_pred <- or_pred
plot(cv_pred-lm_pred)
plot(sl_pred-lm_pred)
# truth
parY = c(1, 1, 1, 0); parU=c(1,1,1,0)
truth <- parY[1]*(parU[1] + parU[2]*A + parU[3]*X + parU[4]*A*X) + parY[2]*M + parY[3]*X + parY[4]*M*X
load("/Users/apple/Library/CloudStorage/Dropbox/Front-door_Anna/code/0-continuous.Rdata")
attach(data, warn.conflicts=FALSE)
dat_MAX <- data.frame(M,A=A,X)
# truth
parY = c(1, 1, 1, 0); parU=c(1,1,1,0)
truth <- parY[1]*(parU[1] + parU[2]*A + parU[3]*X + parU[4]*A*X) + parY[2]*M + parY[3]*X + parY[4]*M*X
# regression
lm_pred <- predict(lm(Y~.,data=dat_MAX))
# CF
lib = c("SL.glm", "SL.gam",
"SL.ranger",
"SL.svm","SL.mean")
or_fit <- CV.SuperLearner(Y=Y, X=dat_MAX, family = gaussian(), V = K, SL.library = lib, control = list(saveFitLibrary=T),saveAll = T)
or_fit <- CV.SuperLearner(Y=Y, X=dat_MAX, family = gaussian(), V = 5, SL.library = lib, control = list(saveFitLibrary=T),saveAll = T)
cv_pred <- or_fit$SL.predict
# SL
or_fit <- SuperLearner(Y=Y, X=dat_MAX, family = gaussian(),SL.library = lib)
sl_pred <- predict(or_fit)[[1]] %>% as.vector()
plot(cv_pred-truth)
plot(sl_pred-truth)
plot(lm_pred-truth)
plot(truth)
head(dat_MAX)
lm(Y~.,data=dat_MAX)
parY[1]*parU[1]
dat_MAX <- data.frame(M,A=factor(A),X)
lm(Y~.,data=dat_MAX)
plot(cv_pred-truth)
plot(sl_pred-truth)
plot(lm_pred-truth)
head(dat_MAX$A[which((lm_pred-truth)<0)])
head(dat_MAX$A[which((lm_pred-truth)<0)],20)
plot(cv_pred-truth)
plot(sl_pred-truth)
plot(lm_pred-truth)
